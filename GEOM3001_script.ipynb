{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542692d6-7b27-4076-9786-30db42c87b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datacube\n",
    "from datacube.utils import cog\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datacube.utils.masking import make_mask\n",
    "from datacube.utils.masking import mask_invalid_data\n",
    "from odc.algo import mask_cleanup\n",
    "import sys\n",
    "sys.path.insert(1, '../Tools/')\n",
    "from dea_tools.datahandling import load_ard\n",
    "from dea_tools.plotting import rgb, display_map\n",
    "from dea_tools.bandindices import calculate_indices\n",
    "from dea_tools.dask import create_local_dask_cluster\n",
    "from dea_tools.spatial import xr_vectorize\n",
    "import scipy.ndimage\n",
    "# Create local dask cluster to improve data load time\n",
    "client = create_local_dask_cluster(return_client=True)\n",
    "import scipy.ndimage\n",
    "import xarray\n",
    "import numpy\n",
    "import datacube\n",
    "from datacube.utils.masking import make_mask\n",
    "from datacube.utils.masking import mask_invalid_data\n",
    "from odc.algo import mask_cleanup\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../Tools/')\n",
    "from dea_tools.plotting import rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd98c87-4eac-4471-9904-54652c80b28a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"bais2_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9598ac-d4f9-4f60-a199-55cf732532b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the area of interest\n",
    "latitude = (-28.1218076, -28.2168763)\n",
    "longitude = (153.1025233, 153.2382923)\n",
    "\n",
    "# Set the range of dates for the complete sample\n",
    "time = ('1999', '2024')\n",
    "# Compute the bounding box for the study area\n",
    "study_area_lat = (-28.1218076, -28.2168763)\n",
    "study_area_lon = (153.1025233, 153.2382923)\n",
    "\n",
    "display_map(x=study_area_lon, y=study_area_lat, margin=-0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b00cd1-9138-4a27-91e9-74cc7425f7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fire event date\n",
    "fire_date = '2019-09-06'\n",
    "\n",
    "# Length of baseline period\n",
    "fire_length = '3 months'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd4392-7391-41f8-80aa-415566c60880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define dates for loading data\n",
    "if fire_length == '12 months':\n",
    "    time_step = timedelta(days=365)\n",
    "if fire_length == '6 months':\n",
    "    time_step = timedelta(days=182.5)\n",
    "if fire_length == '3 months':\n",
    "    time_step = timedelta(days=91)\n",
    "\n",
    "# Calculate the start and end date for baseline data load\n",
    "start_date_pre = datetime.strftime(\n",
    "    ((datetime.strptime(fire_date, '%Y-%m-%d')) - time_step), '%Y-%m-%d')\n",
    "end_date_pre = datetime.strftime(\n",
    "    ((datetime.strptime(fire_date, '%Y-%m-%d')) - timedelta(days=1)),\n",
    "    '%Y-%m-%d')\n",
    "\n",
    "# Calculate end date for post fire data load\n",
    "start_date_post = datetime.strftime(\n",
    "    ((datetime.strptime(fire_date, '%Y-%m-%d')) + timedelta(days=15)),\n",
    "    '%Y-%m-%d')\n",
    "end_date_post = datetime.strftime(\n",
    "    ((datetime.strptime(fire_date, '%Y-%m-%d')) + timedelta(days=90)),\n",
    "    '%Y-%m-%d')\n",
    "\n",
    "\n",
    "# Print dates\n",
    "print(f'start_date_pre:  {start_date_pre}')\n",
    "print(f'end_date_pre:    {end_date_pre}')\n",
    "print(f'fire_date:       {fire_date}')\n",
    "print(f'start_date_post: {start_date_post}')\n",
    "print(f'end_date_post:   {end_date_post}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58584e4d-dfdb-4707-a398-87c78d829d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Band List\n",
    "#dc_measurements = dc.list_measurements()\n",
    "#dc_measurements.loc[['ga_s2am_ard_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f65dc2a-b6f3-4123-b6a0-c072319c87c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resolution = (-20, 20)\n",
    "measurements = ['nbart_red_edge_2', 'nbart_red_edge_3', 'nbart_nir_2', 'nbart_red', 'nbart_swir_3', 'nbart_nir_1', 'nbart_green', 'nbart_blue','fmask' ]\n",
    "min_gooddata = 0.9\n",
    "output_crs = 'EPSG:9473'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19812cbf-37f9-466d-86e0-b9daa1864526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load all data in baseline period available from ARD data\n",
    "dsbaseline = load_ard(dc=dc,\n",
    "                        products=['ga_s2am_ard_3', 'ga_s2bm_ard_3'],\n",
    "                        x=study_area_lon,\n",
    "                        y=study_area_lat,\n",
    "                        time=(start_date_pre, end_date_pre),\n",
    "                        measurements=measurements,\n",
    "                        min_gooddata=min_gooddata,\n",
    "                        resampling={\n",
    "                         \"fmask\": \"nearest\",\n",
    "                          \"*\": \"bilinear\"},\n",
    "                        output_crs=output_crs,\n",
    "                        resolution=resolution,\n",
    "                        group_by='solar_day')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b48e764-8262-4964-9738-2904a23bcc24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#selecting best timestep\n",
    "rgb(dsbaseline,col=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e89b82-b4d0-4fe1-aaa4-cf5b00e5b558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dsbaseline.fmask.attrs[\"flags_definition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f69f61-9197-48cf-9fd3-b60533920413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dsbaseline.fmask.plot(col=\"time\", col_wrap=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15635742-43d2-49f9-bda1-1531bb0cb097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the mask based on \"valid\" pixels\n",
    "clear_mask = make_mask(dsbaseline.fmask, fmask=\"valid\")\n",
    "#clear_mask.plot(col=\"time\", col_wrap=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71300139-d600-4b61-9fae-7a7643b63b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the mask\n",
    "baseclear = dsbaseline.where(clear_mask)\n",
    "#rgb(baseclear, col=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcbfc29-628a-43e5-b71a-be2659ec1806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datacube\n",
    "import geopandas as gpd\n",
    "import odc.geo.xr\n",
    "from odc.geo.geom import Geometry\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"../Tools/\")\n",
    "from dea_tools.plotting import rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647cf1e9-52bc-49a3-8300-e5e0fb7dfe5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "polygon_to_drill = (\n",
    "    \"lamington_boundary.geojson\"\n",
    ")\n",
    "# Read vector file\n",
    "polygon_to_drill = gpd.read_file(polygon_to_drill)\n",
    "\n",
    "# Check that the polygon loaded as expected. We'll just print the first 3 rows to check\n",
    "#polygon_to_drill.head(3)\n",
    "# Select polygon\n",
    "shapely_geometry = polygon_to_drill.iloc[0].geometry\n",
    "\n",
    "# Convert to Geometry object with CRS information\n",
    "geom = Geometry(geom=shapely_geometry, crs=polygon_to_drill.crs)\n",
    "geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f6c92-ab4e-40c2-85d3-37acc0015e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mask out all pixels outside of our polygon:\n",
    "base_fullymasked = baseclear.odc.mask(poly=geom)\n",
    "#rgb(base_fullymasked, col=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29865f-841e-4d67-9263-4c58e66e32d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb4540-9056-4997-a75b-c867a2626d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the timesteps to visualise\n",
    "best_timestep = 19\n",
    "\n",
    "# Generate RGB plot of best timestamp\n",
    "rgb(base_fullymasked, index=[best_timestep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884ce72-41e2-4e0c-addb-3bb6d2727926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#defining bais2 calculation\n",
    "B06=base_fullymasked.nbart_red_edge_2/10000\n",
    "B07=base_fullymasked.nbart_red_edge_3/10000\n",
    "B8A=base_fullymasked.nbart_nir_2/10000\n",
    "B04=base_fullymasked.nbart_red/10000\n",
    "B12=base_fullymasked.nbart_swir_3/10000\n",
    "\n",
    "#applying the calculation to the bands in ds then adding it as a variable in that ds\n",
    "base_fullymasked['BAIS2'] = (1-((B06*B07*B8A)/B04)**0.5)*((B12-B8A)/((B12+B8A)**0.5)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b7731-fcce-4dce-a90d-5586537bfd3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#selecting the baseline timestep\n",
    "baseline_BAI = base_fullymasked.BAIS2.isel(time=19)\n",
    "# Select RGB baseline\n",
    "baseline_image = base_fullymasked.isel(time=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa1b8ab-a896-4406-a4af-b7188c7581bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plotting histogram of baseline_BAI, used to set vmin and vmax\n",
    "plt.hist(baseline_BAI.data.flatten(),bins=np.arange(-1,1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4885585f-ba84-4d19-9e4e-ce8535b244a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plotting the baseline comparison\n",
    "# Set up subplots for baseline\n",
    "f, axarr = plt.subplots(1, 2, figsize=(15, 7), squeeze=False)\n",
    "\n",
    "# Visualise post-fire image as a true colour image\n",
    "rgb(baseline_image, \n",
    "    bands=['nbart_red', 'nbart_green', 'nbart_blue'], \n",
    "    ax=axarr[0, 0])\n",
    "axarr[0, 0].set_title('Pre-fire RGB')\n",
    "axarr[0, 0].set_xlabel('X coordinate')\n",
    "axarr[0, 0].set_ylabel('Y coordinate')\n",
    "\n",
    "# Visualise post-fire image as BAI image\n",
    "baseline_BAI.plot(cmap='RdBu', vmin=-1, vmax=1, ax=axarr[0, 1])\n",
    "axarr[0, 1].set_title('Pre-fire BAIS2')\n",
    "axarr[0, 1].yaxis.set_visible(True)\n",
    "axarr[0, 1].set_xlabel('X coordinate');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90539e86-3251-4cf4-8809-a7d22c2970ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Preparing the post-fire images\n",
    "#Load post-fire data from Sentinel-2A and 2B\n",
    "# Load all data in baseline period available from ARD data\n",
    "dspost = load_ard(dc=dc,\n",
    "                        products=['ga_s2am_ard_3', 'ga_s2bm_ard_3'],\n",
    "                        x=study_area_lon,\n",
    "                        y=study_area_lat,\n",
    "                        time=('2019-12-5', '2020-01-30'),\n",
    "                        measurements=measurements,\n",
    "                        min_gooddata=min_gooddata,\n",
    "                        resampling={\n",
    "                         \"fmask\": \"nearest\",\n",
    "                          \"*\": \"bilinear\"},\n",
    "                        output_crs=output_crs,\n",
    "                        resolution=resolution,\n",
    "                        group_by='solar_day')\n",
    "# Load all data in baseline period available from ARD data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208378f-83b5-4f62-8552-9798e74b3ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#selecting post fire image\n",
    "rgb(dspost,col=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215a5a1-f452-44ea-a4b0-a9702d5c862b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dspost.fmask.attrs[\"flags_definition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28befa-6d98-4ff4-a8ac-52ecbfc2d481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clear_mask_post = make_mask(dspost.fmask, fmask=\"valid\")\n",
    "#clear_mask.plot(col=\"time\", col_wrap=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7f80a-c68e-4628-a4c0-e5016d3184a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "postclear = dspost.where(clear_mask_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e08c7-9355-4384-9e6d-55b76a537ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mask out all pixels outside of our polygon:\n",
    "post_fullymasked = postclear.odc.mask(poly=geom)\n",
    "#rgb(post_fullymasked, col=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0dc026-d145-4078-b517-592f0de8a38d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the timesteps to visualise\n",
    "best_timestep1 = 6\n",
    "\n",
    "# Generate RGB plot of best timestamp\n",
    "rgb(post_fullymasked, index=[best_timestep1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8bb7d-dd78-4108-a730-eb22a70f769f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#defining bais2 calculation\n",
    "B06=post_fullymasked.nbart_red_edge_2/10000\n",
    "B07=post_fullymasked.nbart_red_edge_3/10000\n",
    "B8A=post_fullymasked.nbart_nir_2/10000\n",
    "B04=post_fullymasked.nbart_red/10000\n",
    "B12=post_fullymasked.nbart_swir_3/10000\n",
    "\n",
    "#applying the calculation to the bands in ds then adding it as a variable in that ds\n",
    "post_fullymasked['BAIS2'] = (1-((B06*B07*B8A)/B04)**0.5)*((B12-B8A)/((B12+B8A)**0.5)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01695e26-1c3e-476d-a427-defb3e9fe923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "post_fullymasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c59d1-f5e6-4543-965f-7a6960dd080a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#selecting the baseline timestep\n",
    "post_BAI = post_fullymasked.BAIS2.isel(time=best_timestep1)\n",
    "# Select RGB baseline\n",
    "post_image = post_fullymasked.isel(time=best_timestep1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633307a3-afc3-4d26-9fbe-006c07340368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#post_BAI.data.flatten()\n",
    "#plotting histogram of baseline_BAI, used to set vmin and vmax\n",
    "plt.hist(post_BAI.data.flatten(),bins=np.arange(-1,1.4,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b6e1c-82e9-4280-b4f1-0381e9374257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up subplots\n",
    "f, axarr = plt.subplots(1, 2, figsize=(15, 7), squeeze=False)\n",
    "\n",
    "# Visualise post-fire image as a true colour image\n",
    "rgb(post_image, \n",
    "    bands=['nbart_red', 'nbart_green', 'nbart_blue'], \n",
    "    ax=axarr[0, 0])\n",
    "axarr[0, 0].set_title('Post-fire RGB')\n",
    "axarr[0, 0].set_xlabel('X coordinate')\n",
    "axarr[0, 0].set_ylabel('Y coordinate')\n",
    "\n",
    "# Visualise post-fire image as BAI image\n",
    "post_BAI.plot(cmap='RdBu', vmin=0.5, vmax=0.8, ax=axarr[0, 1])\n",
    "axarr[0, 1].set_title('Post-fire BAIS2')\n",
    "axarr[0, 1].yaxis.set_visible(True)\n",
    "axarr[0, 1].set_xlabel('X coordinate');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779347e-316b-498d-ab6d-25ef5cee33db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating delta of baseline and post images\n",
    "delta_BAI = post_BAI - baseline_BAI\n",
    "\n",
    "# Visualise dBAIS2 image\n",
    "delta_BAI.plot(cmap='RdBu_r', vmin=-0.3, vmax=0.6, figsize=(11, 9))\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc1c8c5-5f1a-4633-8d73-2df2a5c03bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "plt.hist(delta_BAI.data.flatten(),bins=np.arange(-0.75,0.6,0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e360d2-f6c3-4c73-851e-b32a0e96504c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bam = delta_BAI\n",
    "# Create a figure to plot the chosen fire severity index\n",
    "f, axarr = plt.subplots(\n",
    "    1, 2, figsize=(15, 10), squeeze=False, gridspec_kw={\"width_ratios\": [1, 5]}\n",
    ")\n",
    "\n",
    "# Calculate and round the dBAI dataarray value range to set determine the plots colourmap range\n",
    "bam_BAI_min = round(float(bam.quantile(0.005)), 1)\n",
    "bam_BAI_max = round(float(bam.quantile(0.995)), 1)\n",
    "\n",
    "# PLot the dBAI dataarray on the second subplot of the above figure\n",
    "bam.plot(cmap=\"RdBu_r\", vmin=bam_BAI_min, vmax=bam_BAI_max, ax=axarr[(0, 1)])\n",
    "\n",
    "# Plot a histogram of dBAI values in the first figure subplot.\n",
    "# Calculate a colourmap from the dataarray plot by iterating through individual histogram patches\n",
    "cm = plt.colormaps.get_cmap(\"RdBu_r\")\n",
    "\n",
    "n, bins, patches = xr.plot.hist(\n",
    "    darray=bam,\n",
    "    bins=np.arange(bam_BAI_min, bam_BAI_max + 0.05, 0.05),\n",
    "    align=\"mid\",\n",
    "    orientation=\"horizontal\",\n",
    "    ec=\"black\",\n",
    "    yticks=(np.arange(bam_BAI_min - 0.05, bam_BAI_max + 0.05, step=0.05)),\n",
    "    ax=axarr[(0, 0)],\n",
    ")\n",
    "\n",
    "# Match the colour scale of the histogram to that used in the map plot.\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= max(col)\n",
    "for c, p in zip(col, patches):\n",
    "    plt.setp(p, \"facecolor\", cm(c))\n",
    "\n",
    "# Set titles for each subplot\n",
    "axarr[0, 0].set_title('dBAIS2' + \" Histogram\")\n",
    "axarr[0, 1].set_xlabel('X Coordinate')\n",
    "axarr[0, 1].set_ylabel('Y Coordinate')\n",
    "axarr[0, 1].set_title(\n",
    "    'dBAIS2'\n",
    "    + \" measured between \"\n",
    "    + str(baseline_BAI.time.values)[:10]\n",
    "    + \" - \"\n",
    "    + str(post_BAI.time.values)[:10]\n",
    ")\n",
    "\n",
    "# Set the x-axis label and number of x-axis ticks for the histogram plot\n",
    "axarr[0, 0].set_xlabel('dBAIS2' + \" count\")\n",
    "axarr[0, 0].xaxis.set_major_locator(plt.MaxNLocator(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad88c84-e823-4881-bf5b-d081f813c944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create pixel classes\n",
    "unburnt = delta_BAI < 0.2\n",
    "low = (delta_BAI > 0.2) & (delta_BAI < 0.3)\n",
    "moderate = (delta_BAI > 0.3) & (delta_BAI < 0.4)\n",
    "high = (delta_BAI > 0.4) & (delta_BAI < 0.45)\n",
    "extreme = delta_BAI > 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e510c-2706-4734-85e6-7d53da41ffb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Assuming 'delta_BAI' is a NumPy array\n",
    "#delta_BAI = xr.DataArray(delta_BAI, dims=['636', '739'])  # Replace 'dim1', 'dim2' with actual dimension names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c5fa1-1627-42a1-9cca-5e34fae9a1ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multiply by 1000 to match the classification ranges\n",
    "BAI_scaled = delta_BAI * 1000\n",
    "\n",
    "# Define the conditions and corresponding values using EU classes (https://forest-fire.emergency.copernicus.eu/about-effis/technical-background/fire-severity)\n",
    "conditions = [\n",
    "    (BAI_scaled < 200),\n",
    "    (BAI_scaled >= 200) & (BAI_scaled < 300),\n",
    "    (BAI_scaled >= 300) & (BAI_scaled < 400),\n",
    "    (BAI_scaled >= 400) & (BAI_scaled < 450),\n",
    "    (BAI_scaled >= 450)\n",
    "]\n",
    "\n",
    "values = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create a new DataArray with the classified values\n",
    "BAI_classified = xr.DataArray(\n",
    "    np.select(conditions, values),\n",
    "    coords=delta_BAI.coords,\n",
    "    dims=delta_BAI.dims,\n",
    "    name='BAI_Class'\n",
    ")\n",
    "\n",
    "# Add attributes for class descriptions\n",
    "BAI_classified.attrs['classes'] = {\n",
    "    1: 'Unburned or Regrowth',\n",
    "    2: 'Low severity',\n",
    "    3: 'Moderate severity',\n",
    "    4: 'High severity',\n",
    "    5: 'Extreme severity'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b8452-5368-43b2-961b-15939d8369f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install matplotlib-scalebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4b8f3-49c8-479a-901d-ce7e355738d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "\n",
    "# Assuming you have already created BAI_classified as in the previous example\n",
    "\n",
    "# Define colors for each class\n",
    "colors = ['olivedrab', 'yellow', 'orange', 'red', 'darkred']\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "# Set up the colorbar ticks and boundaries\n",
    "bounds = np.arange(0.5, 6.5, 1)  # 0.5, 1.5, 2.5, 3.5, 4.5, 5.5\n",
    "norm = BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Plot the data\n",
    "im = ax.imshow(BAI_classified, cmap=cmap, norm=norm)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(im, ax=ax, orientation='vertical', pad=0.02)\n",
    "\n",
    "# Set colorbar ticks and labels\n",
    "cbar.set_ticks(np.arange(1, 6))\n",
    "cbar.set_ticklabels(['Unburned or Regrowth', 'Low severity', 'Moderate severity', \n",
    "                     'High severity', 'Extreme severity'])\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_title('Burn Severity Map (dBAIS2)', fontsize=16)\n",
    "\n",
    "# Remove x and y ticks\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Add a scale bar\n",
    "scalebar = ScaleBar(1, location='lower right')  # You can adjust the value to represent the map scale\n",
    "ax.add_artist(scalebar)\n",
    "\n",
    "# Add a north arrow\n",
    "# Define the position and size of the north arrow\n",
    "arrow_x = 0.9  # x position (in axis coordinates, from 0 to 1)\n",
    "arrow_y = 0.9  # y position (in axis coordinates, from 0 to 1)\n",
    "arrow_length = 0.1  # Length of the arrow in axis coordinates\n",
    "\n",
    "x, y, arrow_length = 0.05, 0.95, 0.07\n",
    "ax.annotate('N', xy=(x, y), xytext=(x, y-arrow_length),\n",
    "            arrowprops=dict(facecolor='black', width=5, headwidth=15),\n",
    "            ha='center', va='center', fontsize=20,\n",
    "            xycoords=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f395f37-e446-4e69-8083-39422dc3dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for calculating burnt area (using your existing constants)\n",
    "pixel_length = resolution[1]  # in metres\n",
    "m_per_km = 1000  # conversion from metres to kilometres\n",
    "area_per_pixel = pixel_length ** 2 / m_per_km ** 2\n",
    "\n",
    "# Initialize dictionary to store results\n",
    "severity_stats = {}\n",
    "\n",
    "# Calculate area for each class\n",
    "for class_value, class_name in BAI_classified.attrs['classes'].items():\n",
    "    # Calculate area for this class\n",
    "    class_pixels = (BAI_classified == class_value).sum()\n",
    "    class_area = class_pixels * area_per_pixel\n",
    "    \n",
    "    # Store in dictionary\n",
    "    severity_stats[class_name] = class_area.item()\n",
    "\n",
    "# Calculate total area (excluding NaN)\n",
    "total_valid_area = sum(severity_stats.values())\n",
    "\n",
    "# Create and print formatted results\n",
    "print(\"\\nBurn Severity Statistics:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Severity Class':<25} {'Area (km²)':<15} {'Percentage':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for class_name, area in severity_stats.items():\n",
    "    percentage = (area / total_valid_area) * 100\n",
    "    print(f\"{class_name:<25} {area:>8.2f} km²    {percentage:>8.1f}%\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total area':<25} {total_valid_area:>8.2f} km²    {100:>8.1f}%\")\n",
    "\n",
    "# Optional: Calculate NaN area\n",
    "nan_area = BAI_classified.isnull().sum() * area_per_pixel\n",
    "print(f\"{'No data (NaN) area':<25} {nan_area.item():>8.2f} km²\")\n",
    "\n",
    "# Previous code remains the same until the results section\n",
    "\n",
    "# Calculate total burnt area (excluding unburned class)\n",
    "total_burnt = sum(area for class_name, area in severity_stats.items() \n",
    "                 if class_name != 'Unburned or Regrowth')\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total area':<25} {total_valid_area:>8.2f} km²    {100:>8.1f}%\")\n",
    "print(f\"{'No data (NaN) area':<25} {nan_area.item():>8.2f} km²\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total burnt area':<25} {total_burnt:>8.2f} km²    {(total_burnt/total_valid_area)*100:>8.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e55d6b-5af6-4f1f-9f3a-3cf5a4d6d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a comparison\n",
    "comparison = xr.Dataset({\n",
    "    'simple_burnt': burnt,\n",
    "    'severity_burnt': BAI_classified > 1  # All classes except unburned\n",
    "})\n",
    "\n",
    "# Calculate areas for comparison\n",
    "simple_burnt_area = burnt.sum() * area_per_pixel\n",
    "severity_burnt_area = (BAI_classified > 1).sum() * area_per_pixel\n",
    "\n",
    "# Calculate the difference map\n",
    "difference = comparison.simple_burnt != (BAI_classified > 1)\n",
    "difference_area = difference.sum() * area_per_pixel\n",
    "\n",
    "print(\"Comparison of burnt area calculations:\")\n",
    "print(f\"Simple threshold method: {simple_burnt_area.item():.2f} km²\")\n",
    "print(f\"Severity classes method: {severity_burnt_area.item():.2f} km²\")\n",
    "print(f\"Area of disagreement:    {difference_area.item():.2f} km²\")\n",
    "\n",
    "# Let's see where these values differ\n",
    "threshold_scaled = threshold * 1000  # Your original threshold scaled\n",
    "print(f\"\\nThreshold analysis:\")\n",
    "print(f\"Original threshold: {threshold}\")\n",
    "print(f\"Scaled threshold:  {threshold_scaled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb3b02-fcb8-4ed8-a628-09c6c085e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the thresholds\n",
    "aligned_threshold = 0.2  # Your original threshold\n",
    "\n",
    "# For burn area calculation\n",
    "burnt = delta_BAI > aligned_threshold\n",
    "\n",
    "# For severity calculation (using aligned thresholds)\n",
    "BAI_scaled = delta_BAI * 1000\n",
    "\n",
    "# Define the conditions ensuring alignment with burn area\n",
    "conditions = [\n",
    "    (BAI_scaled <= aligned_threshold * 1000),  # Unburned\n",
    "    (BAI_scaled > aligned_threshold * 1000) & (BAI_scaled < 300),\n",
    "    (BAI_scaled >= 300) & (BAI_scaled < 400),\n",
    "    (BAI_scaled >= 400) & (BAI_scaled < 450),\n",
    "    (BAI_scaled >= 450)\n",
    "]\n",
    "\n",
    "values = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create aligned severity classification\n",
    "BAI_classified_aligned = xr.DataArray(\n",
    "    np.select(conditions, values),\n",
    "    coords=delta_BAI.coords,\n",
    "    dims=delta_BAI.dims,\n",
    "    name='BAI_Class'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf267cd0-9ad5-479f-9d51-74677c760c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#masking the final burn scar\n",
    "\n",
    "# Set threshold\n",
    "threshold = 0.2\n",
    "\n",
    "\n",
    "# Apply threshold\n",
    "burnt = delta_BAI > threshold\n",
    "\n",
    "delta_BAI['burnt'] = delta_BAI < threshold\n",
    "#delta_BAIt = delta_BAI.burnt\n",
    "\n",
    "\n",
    "total_burn = delta_BAI.where(burnt==1)\n",
    "# Visualise dBAI image\n",
    "\n",
    "total_burn.plot(cmap='inferno', figsize=(11, 9), vmax=0.6, vmin=-0.2)\n",
    "plt.title('dBAIS2 Burn Area')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f6da0-3349-495f-b990-d74e0af3c553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "from skimage import morphology\n",
    "\n",
    "sys.path.insert(1, \"../Tools/\")\n",
    "from dea_tools.bandindices import calculate_indices\n",
    "from dea_tools.datahandling import load_ard\n",
    "from dea_tools.plotting import display_map, rgb, plot_variable_images\n",
    "from dea_tools.spatial import xr_rasterize, xr_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277382c-dc10-468c-b246-f3d9835ea2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#salt/pepper clean?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27682438-0d1c-48ff-b34e-f0bce1a5e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Burnt Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc84200-70c7-40b1-8344-31d33c1a4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for calculating burnt area\n",
    "pixel_length = resolution[1]  # in metres\n",
    "m_per_km = 1000  # conversion from metres to kilometres\n",
    "\n",
    "# Area per pixel\n",
    "area_per_pixel = pixel_length ** 2 / m_per_km ** 2\n",
    "\n",
    "# Calculate areas\n",
    "unburnt_area = (delta_BAI <= threshold).sum() * area_per_pixel\n",
    "burnt_area = burnt.sum() * area_per_pixel\n",
    "not_nan_area = delta_BAI.notnull().sum() * area_per_pixel\n",
    "nan_area = delta_BAI.isnull().sum() * area_per_pixel\n",
    "total_area = unburnt_area + burnt_area + nan_area\n",
    "\n",
    "print(f'Unburnt area:            {unburnt_area.item():.2f} km^2')\n",
    "print(f'Burnt area:              {burnt_area.item():.2f} km^2')\n",
    "print(f'Nan area:                {nan_area.item():.2f} km^2')\n",
    "print(f'Total area (no nans):    {not_nan_area.item():.2f} km^2')\n",
    "print(f'Total area (with nans):  {total_area.item():.2f} km^2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df3051-ab41-4cac-96d1-fcd8d05d11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for calculating burnt area\n",
    "pixel_length = resolution[1]  # in metres\n",
    "m_per_km = 1000  # conversion from metres to kilometres\n",
    "\n",
    "# Area per pixel\n",
    "area_per_pixel = pixel_length ** 2 / m_per_km ** 2\n",
    "\n",
    "# Calculate burnt area (only where burnt == 1)\n",
    "burnt_pixels = burnt.sum()\n",
    "burnt_area = burnt_pixels * area_per_pixel\n",
    "\n",
    "# Calculate total area and unburnt area for context\n",
    "total_pixels = burnt.notnull().sum()\n",
    "total_area = total_pixels * area_per_pixel\n",
    "unburnt_area = (total_area - burnt_area)\n",
    "\n",
    "print(\"\\nBurn Area Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total burnt area:        {burnt_area.item():.2f} km²\")\n",
    "print(f\"Total unburnt area:      {unburnt_area.item():.2f} km²\")\n",
    "print(f\"Total area:              {total_area.item():.2f} km²\")\n",
    "print(f\"Percentage burnt:        {(burnt_area.item()/total_area.item()*100):.1f}%\")\n",
    "\n",
    "# Optional: Add number of pixels for verification\n",
    "print(\"\\nPixel Counts:\")\n",
    "print(f\"Burnt pixels:            {burnt_pixels.item():,}\")\n",
    "print(f\"Total valid pixels:      {total_pixels.item():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768fca7-2c2a-4014-b309-75628b39fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download as shapefile\n",
    "from dea_tools.spatial import xr_rasterize, xr_vectorize\n",
    "# Convert the burnt area from raster to vector format\n",
    "gdf = xr_vectorize(\n",
    "    da=total_burn, output_path= \"total_me.shp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b15ba-9ba3-4539-bd38-551d7d42111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download as shapefile\n",
    "from dea_tools.spatial import xr_rasterize, xr_vectorize\n",
    "# Convert the burnt area from raster to vector format\n",
    "gdf = xr_vectorize(\n",
    "    da=BAI_classified, output_path= \"sevvv.shp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083aed2-08e3-432b-976c-452ecb23e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorise\n",
    "#polygons = xr_vectorize(total_burn)\n",
    "#polygons[\"area\"] = polygons.area\n",
    "#polygons[\"size\"] = np.where(polygons[\"area\"] < 50000, 1, 2)  # se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f707679-aaeb-4a59-943c-306a592c9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set specific EPSG code (replace 32754 with your correct EPSG code)\n",
    "total_burn.rio.write_crs('EPSG:9473', inplace=True)\n",
    "\n",
    "# Then export\n",
    "write_cog(geo_im=total_burn,\n",
    "          fname='tburnnnn.tif',\n",
    "          overwrite=True,\n",
    "          nodata=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5d85c-98f4-4219-ba57-90b6a2820292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export burnt area map to test\n",
    "from datacube.utils.cog import write_cog\n",
    "# Write GeoTIFF\n",
    "write_cog(geo_im=total_burn,\n",
    "          fname='tburn.tif',\n",
    "          overwrite=True,\n",
    "          nodata=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed622370-edc9-4fce-9862-051c2b575577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required library\n",
    "import geopandas as gpd\n",
    "\n",
    "# Read the shapefile\n",
    "qpws = gpd.read_file('qpws_layer.shp')\n",
    "\n",
    "# Print basic information about the layer\n",
    "print(\"QPWS Layer Information:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"CRS: {qpws.crs}\")\n",
    "print(f\"Number of features: {len(qpws)}\")\n",
    "print(f\"Columns available: {qpws.columns.tolist()}\")\n",
    "\n",
    "# Calculate areas in square kilometers\n",
    "qpws['area_km2'] = qpws.geometry.area / 1_000_000  # converts m² to km²\n",
    "\n",
    "# Calculate total area\n",
    "total_area = qpws['area_km2'].sum()\n",
    "\n",
    "print(\"\\nArea Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total area: {total_area:.2f} km²\")\n",
    "\n",
    "# If you have multiple features and want to see individual areas:\n",
    "if len(qpws) > 1:\n",
    "    print(\"\\nIndividual Feature Areas:\")\n",
    "    for idx, area in enumerate(qpws['area_km2']):\n",
    "        print(f\"Feature {idx + 1}: {area:.2f} km²\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6e49b-48b9-402d-84ba-f35a9e6d9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Read the shapefile\n",
    "qpws = gpd.read_file('qpws_layer.shp')\n",
    "\n",
    "# Project to EPSG:9473\n",
    "qpws_projected = qpws.to_crs(epsg=9473)\n",
    "\n",
    "# Calculate areas in square kilometers\n",
    "qpws_projected['area_km2'] = qpws_projected.geometry.area / 1_000_000\n",
    "\n",
    "# Print basic information\n",
    "print(\"QPWS Layer Information:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"CRS: {qpws_projected.crs}\")\n",
    "print(f\"Number of features: {len(qpws_projected)}\")\n",
    "print(f\"Columns available: {qpws_projected.columns.tolist()}\")\n",
    "\n",
    "# Area Statistics\n",
    "print(\"\\nArea Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total area: {qpws_projected['area_km2'].sum():.2f} km²\")\n",
    "\n",
    "# Severity Statistics\n",
    "print(\"\\nSeverity Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "severity_stats = qpws_projected.groupby('Severity').agg({\n",
    "    'area_km2': ['sum', 'count']\n",
    "}).round(2)\n",
    "severity_stats.columns = ['Total Area (km²)', 'Number of Features']\n",
    "print(severity_stats)\n",
    "\n",
    "# Individual feature details\n",
    "print(\"\\nDetailed Feature Information:\")\n",
    "print(\"-\" * 40)\n",
    "for idx, row in qpws_projected.iterrows():\n",
    "    print(f\"Feature {idx + 1}:\")\n",
    "    print(f\"Estate Name: {row['ESTATENAME']}\")\n",
    "    print(f\"Severity: {row['Severity']}\")\n",
    "    print(f\"Area: {row['area_km2']:.2f} km²\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Map of areas colored by severity\n",
    "qpws_projected.plot(column='Severity', \n",
    "                   legend=True,\n",
    "                   legend_kwds={'title': 'Severity'},\n",
    "                   cmap='YlOrRd',\n",
    "                   ax=ax1)\n",
    "ax1.set_title('QPWS Areas by Severity')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Bar plot of areas by severity\n",
    "severity_areas = qpws_projected.groupby('Severity')['area_km2'].sum()\n",
    "severity_areas.plot(kind='bar', \n",
    "                   ax=ax2,\n",
    "                   color='orangered')\n",
    "ax2.set_title('Total Area by Severity')\n",
    "ax2.set_xlabel('Severity')\n",
    "ax2.set_ylabel('Area (km²)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Save statistics to a CSV file\n",
    "severity_stats.to_csv('severity_statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068bdc1b-81f8-4010-ac68-a1faa46daa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project QPWS layer\n",
    "qpws = gpd.read_file('qpws_layer.shp')\n",
    "qpws_projected = qpws.to_crs(epsg=9473)\n",
    "\n",
    "# Calculate QPWS (dNBR) area\n",
    "dnbr_area_m2 = qpws_projected.geometry.area.sum()\n",
    "dnbr_ha = dnbr_area_m2 / 10000  # m² to hectares\n",
    "dnbr_km2 = dnbr_area_m2 / 1000000  # m² to km²\n",
    "\n",
    "# Convert BAIS2 burnt area from km² to hectares\n",
    "bais2_km2 = burnt_area.item()  # from your existing calculation\n",
    "bais2_ha = bais2_km2 * 100  # convert km² to hectares\n",
    "\n",
    "# Calculate statistics\n",
    "abs_diff_ha = abs(bais2_ha - dnbr_ha)\n",
    "rel_diff_percent = (abs_diff_ha / bais2_ha) * 100\n",
    "agreement_ratio = min(bais2_ha, dnbr_ha) / max(bais2_ha, dnbr_ha)\n",
    "mean_area_ha = np.mean([bais2_ha, dnbr_ha])\n",
    "std_dev_ha = np.std([bais2_ha, dnbr_ha])\n",
    "cv_percent = (std_dev_ha / mean_area_ha) * 100\n",
    "\n",
    "# Create statistics DataFrame\n",
    "stats = pd.DataFrame({\n",
    "    'Statistic': [\n",
    "        'BAIS2 burnt area',\n",
    "        'QPWS (dNBR) burnt area',\n",
    "        'Absolute Difference',\n",
    "        'Relative Difference',\n",
    "        'Agreement Ratio',\n",
    "        'Mean Burnt Area',\n",
    "        'Standard Deviation',\n",
    "        'Coefficient of Variation (CV)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f'{bais2_ha:.2f} hectares ({bais2_km2:.2f} km²)',\n",
    "        f'{dnbr_ha:.2f} hectares ({dnbr_km2:.2f} km²)',\n",
    "        f'{abs_diff_ha:.2f} hectares',\n",
    "        f'{rel_diff_percent:.2f}%',\n",
    "        f'{agreement_ratio:.4f}',\n",
    "        f'{mean_area_ha:.2f} hectares',\n",
    "        f'{std_dev_ha:.2f} hectares',\n",
    "        f'{cv_percent:.2f}%'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display the statistics table\n",
    "print(\"\\nBurnt Area Statistics Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(stats.to_string(index=False))\n",
    "\n",
    "# Create a visual comparison\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar plot comparison\n",
    "areas = [bais2_ha, dnbr_ha]\n",
    "labels = ['BAIS2', 'QPWS (dNBR)']\n",
    "bars = ax1.bar(labels, areas, color=['skyblue', 'orange'])\n",
    "ax1.set_ylabel('Area (hectares)')\n",
    "ax1.set_title('Burnt Area Comparison')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f} ha',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Save to CSV\n",
    "stats.to_csv('burnt_area_comparison_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd429017-7c41-48a6-a20a-3f4dd77b5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the QPWS data first\n",
    "qpws_severity = qpws_projected.groupby('Severity').agg({\n",
    "    'geometry': lambda x: sum(item.area for item in x)\n",
    "}).reset_index()\n",
    "\n",
    "qpws_severity['Area_km2'] = qpws_severity['geometry'] / 1000000  # convert m² to km²\n",
    "qpws_total_area = qpws_severity['Area_km2'].sum()\n",
    "qpws_severity['Percentage'] = (qpws_severity['Area_km2'] / qpws_total_area) * 100\n",
    "\n",
    "# Get the area values from severity_stats\n",
    "# From your output, we know this is the 'Total Area (km²)' series\n",
    "area_series = severity_stats['Total Area (km²)']\n",
    "my_severity_stats = pd.DataFrame({\n",
    "    'Class': area_series.index,\n",
    "    'Area_km2': area_series.values\n",
    "})\n",
    "my_severity_stats['Percentage'] = (my_severity_stats['Area_km2'] / my_severity_stats['Area_km2'].sum()) * 100\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\nSeverity Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"| Classification | BAIS2 |  | QPWS |  |\")\n",
    "print(\"|---------------|--------|------------|--------|------------|\")\n",
    "print(\"| Severity Class | Area (km²) | Percentage | Area (km²) | Percentage |\")\n",
    "print(\"|---------------|------------|------------|------------|------------|\")\n",
    "\n",
    "# Combine and align the classes\n",
    "class_mapping = {\n",
    "    'Extreme': 'Extreme',\n",
    "    'High': 'High',\n",
    "    'Moderate': 'Moderate',\n",
    "    'Low': 'Low'\n",
    "}\n",
    "\n",
    "for _, row in my_severity_stats.iterrows():\n",
    "    bais_class = row['Class']\n",
    "    qpws_class = class_mapping.get(bais_class, bais_class)\n",
    "    \n",
    "    bais_area = row['Area_km2']\n",
    "    bais_pct = row['Percentage']\n",
    "    \n",
    "    qpws_row = qpws_severity[qpws_severity['Severity'] == qpws_class]\n",
    "    qpws_area = qpws_row['Area_km2'].iloc[0] if len(qpws_row) > 0 else 0\n",
    "    qpws_pct = qpws_row['Percentage'].iloc[0] if len(qpws_row) > 0 else 0\n",
    "    \n",
    "    print(f\"| {bais_class:<13} | {bais_area:>8.2f} | {bais_pct:>8.1f}% | {qpws_area:>8.2f} | {qpws_pct:>8.1f}% |\")\n",
    "\n",
    "total_bais = my_severity_stats['Area_km2'].sum()\n",
    "print(\"|---------------|------------|------------|------------|------------|\")\n",
    "print(f\"| Total | {total_bais:>8.2f} | 100.0% | {qpws_total_area:>8.2f} | 100.0% |\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# BAIS2 Severity Distribution\n",
    "my_severity_stats.plot(kind='bar', x='Class', y='Area_km2', ax=ax1)\n",
    "ax1.set_title('BAIS2 Severity Distribution')\n",
    "ax1.set_xlabel('Severity Class')\n",
    "ax1.set_ylabel('Area (km²)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# QPWS Severity Distribution\n",
    "qpws_severity.plot(kind='bar', x='Severity', y='Area_km2', ax=ax2)\n",
    "ax2.set_title('QPWS Severity Distribution')\n",
    "ax2.set_xlabel('Severity Class')\n",
    "ax2.set_ylabel('Area (km²)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b8e3c-b3b1-477d-81b8-62d92a257344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BAIS2 statistics DataFrame manually with standardized labels\n",
    "bais2_data = {\n",
    "    'Class': [\n",
    "        'Low',           # Was 'Low severity'\n",
    "        'Moderate',      # Was 'Moderate low severity'\n",
    "        'High',          # Was 'Moderate high severity'\n",
    "        'Extreme'        # Was 'High severity'\n",
    "    ],\n",
    "    'Area_km2': [7.28, 5.03, 1.31, 1.37],\n",
    "    'Percentage': [48.6, 33.6, 8.7, 9.1]\n",
    "}\n",
    "my_severity_stats = pd.DataFrame(bais2_data)\n",
    "\n",
    "# Calculate QPWS severity statistics\n",
    "qpws_severity = qpws_projected.groupby('Severity').agg({\n",
    "    'geometry': lambda x: sum(item.area for item in x)\n",
    "}).reset_index()\n",
    "\n",
    "qpws_severity['Area_km2'] = qpws_severity['geometry'] / 1000000  # convert m² to km²\n",
    "qpws_total_area = qpws_severity['Area_km2'].sum()\n",
    "qpws_severity['Percentage'] = (qpws_severity['Area_km2'] / qpws_total_area) * 100\n",
    "\n",
    "# Ensure consistent ordering\n",
    "severity_order = ['Low', 'Moderate', 'High', 'Extreme']\n",
    "my_severity_stats = my_severity_stats.set_index('Class').reindex(severity_order).reset_index()\n",
    "qpws_severity = qpws_severity.set_index('Severity').reindex(severity_order).reset_index()\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\nSeverity Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"| Classification | BAIS2 |  | QPWS |  |\")\n",
    "print(\"|---------------|--------|------------|--------|------------|\")\n",
    "print(\"| Severity Class | Area (km²) | Percentage | Area (km²) | Percentage |\")\n",
    "print(\"|---------------|------------|------------|------------|------------|\")\n",
    "\n",
    "for severity in severity_order:\n",
    "    bais_row = my_severity_stats[my_severity_stats['Class'] == severity].iloc[0]\n",
    "    qpws_row = qpws_severity[qpws_severity['Severity'] == severity].iloc[0]\n",
    "    \n",
    "    bais_area = bais_row['Area_km2']\n",
    "    bais_pct = bais_row['Percentage']\n",
    "    qpws_area = qpws_row['Area_km2']\n",
    "    qpws_pct = qpws_row['Percentage']\n",
    "    \n",
    "    print(f\"| {severity:<13} | {bais_area:>8.2f} | {bais_pct:>8.1f}% | {qpws_area:>8.2f} | {qpws_pct:>8.1f}% |\")\n",
    "\n",
    "total_bais = my_severity_stats['Area_km2'].sum()\n",
    "print(\"|---------------|------------|------------|------------|------------|\")\n",
    "print(f\"| Total | {total_bais:>8.2f} | 100.0% | {qpws_total_area:>8.2f} | 100.0% |\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# BAIS2 Severity Distribution\n",
    "my_severity_stats.plot(kind='bar', x='Class', y='Area_km2', ax=ax1)\n",
    "ax1.set_title('BAIS2 Severity Distribution')\n",
    "ax1.set_xlabel('Severity Class')\n",
    "ax1.set_ylabel('Area (km²)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# QPWS Severity Distribution\n",
    "qpws_severity.plot(kind='bar', x='Severity', y='Area_km2', ax=ax2)\n",
    "ax2.set_title('QPWS Severity Distribution')\n",
    "ax2.set_xlabel('Severity Class')\n",
    "ax2.set_ylabel('Area (km²)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc4390a-07e9-4163-9fae-8627b33b7ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Using the existing severity classes order\n",
    "severity_order = ['Low', 'Moderate', 'High', 'Extreme']\n",
    "\n",
    "# Create confusion matrix based on area distributions\n",
    "def create_area_confusion_matrix(bais2_df, qpws_df):\n",
    "    # Initialize the confusion matrix\n",
    "    n_classes = len(severity_order)\n",
    "    conf_matrix = np.zeros((n_classes, n_classes))\n",
    "    \n",
    "    # Calculate the proportional distribution\n",
    "    total_area = min(bais2_df['Area_km2'].sum(), qpws_df['Area_km2'].sum())\n",
    "    \n",
    "    for i, bais2_class in enumerate(severity_order):\n",
    "        for j, qpws_class in enumerate(severity_order):\n",
    "            bais2_prop = bais2_df[bais2_df['Class'] == bais2_class]['Area_km2'].iloc[0]\n",
    "            qpws_prop = qpws_df[qpws_df['Severity'] == qpws_class]['Area_km2'].iloc[0]\n",
    "            \n",
    "            # Calculate the overlapping area proportion\n",
    "            overlap = min(bais2_prop, qpws_prop)\n",
    "            conf_matrix[i, j] = overlap\n",
    "    \n",
    "    return conf_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = create_area_confusion_matrix(my_severity_stats, qpws_severity)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "conf_df = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=[f'BAIS2 {cls}' for cls in severity_order],\n",
    "    columns=[f'QPWS {cls}' for cls in severity_order]\n",
    ")\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "total_area = conf_matrix.sum()\n",
    "overall_accuracy = np.trace(conf_matrix) / total_area\n",
    "\n",
    "# Calculate per-class metrics\n",
    "user_accuracy = np.diag(conf_matrix) / conf_matrix.sum(axis=1)\n",
    "producer_accuracy = np.diag(conf_matrix) / conf_matrix.sum(axis=0)\n",
    "\n",
    "# Create accuracy report\n",
    "accuracy_df = pd.DataFrame({\n",
    "    'User\\'s Accuracy': user_accuracy,\n",
    "    'Producer\\'s Accuracy': producer_accuracy\n",
    "}, index=severity_order)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_df, annot=True, fmt='.2f', cmap='YlOrRd')\n",
    "plt.title('Area-based Confusion Matrix: BAIS2 vs QPWS Severity Classifications (km²)')\n",
    "plt.ylabel('BAIS2 Classification')\n",
    "plt.xlabel('QPWS Classification')\n",
    "\n",
    "# Print results\n",
    "print(\"\\nConfusion Matrix (km²):\")\n",
    "print(conf_df)\n",
    "print(\"\\nAccuracy Metrics:\")\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.2f}\")\n",
    "print(\"\\nPer-class Accuracies:\")\n",
    "print(accuracy_df)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional visualization: Normalized confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "norm_conf_matrix = conf_matrix / conf_matrix.sum()\n",
    "norm_conf_df = pd.DataFrame(\n",
    "    norm_conf_matrix,\n",
    "    index=[f'BAIS2 {cls}' for cls in severity_order],\n",
    "    columns=[f'QPWS {cls}' for cls in severity_order]\n",
    ")\n",
    "\n",
    "sns.heatmap(norm_conf_df, annot=True, fmt='.2%', cmap='YlOrRd')\n",
    "plt.title('Normalized Confusion Matrix: BAIS2 vs QPWS Severity Classifications')\n",
    "plt.ylabel('BAIS2 Classification')\n",
    "plt.xlabel('QPWS Classification')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf89e84-056a-409b-a87e-e6a06b4ff859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine what we have\n",
    "print(\"\\nQPWS Data:\")\n",
    "print(type(qpws_projected))\n",
    "print(qpws_projected.head())\n",
    "print(\"\\nQPWS CRS:\")\n",
    "print(qpws_projected.crs)\n",
    "\n",
    "# Try to find BAIS2 spatial data\n",
    "# List all variables in your workspace that might contain 'bais' or related terms\n",
    "import gc\n",
    "variables = [var for var in dir() if not var.startswith('_')]\n",
    "print(\"\\nAvailable variables:\")\n",
    "for var in variables:\n",
    "    obj = locals()[var]\n",
    "    if isinstance(obj, (pd.DataFrame, gpd.GeoDataFrame)):\n",
    "        print(f\"\\n{var}:\")\n",
    "        print(type(obj))\n",
    "        print(obj.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6c1c6-4918-44d7-9d61-eb5b60ea654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio import features\n",
    "import numpy as np\n",
    "from rasterio.transform import from_bounds\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# First, let's set up our raster parameters\n",
    "resolution = 20\n",
    "\n",
    "# Get the bounds of our study area\n",
    "bounds = qpws_projected.total_bounds\n",
    "width = int((bounds[2] - bounds[0]) / resolution)\n",
    "height = int((bounds[3] - bounds[1]) / resolution)\n",
    "transform = from_bounds(*bounds, width, height)\n",
    "\n",
    "# Create raster for QPWS data\n",
    "qpws_raster = np.zeros((height, width))\n",
    "severity_mapping = {'Low': 1, 'Moderate': 2, 'High': 3, 'Extreme': 4}\n",
    "\n",
    "# Rasterize QPWS data\n",
    "shapes = ((geom, severity_mapping[value]) \n",
    "          for geom, value in zip(qpws_projected.geometry, qpws_projected.Severity))\n",
    "qpws_raster = features.rasterize(shapes=shapes, \n",
    "                                out_shape=(height, width),\n",
    "                                transform=transform,\n",
    "                                fill=0)\n",
    "\n",
    "# Create raster for BAIS2 data\n",
    "bais2_raster = features.rasterize(\n",
    "    [(geom, value) for geom, value in zip(total_burn.geometry, total_burn.attribute)],\n",
    "    out_shape=(height, width),\n",
    "    transform=transform,\n",
    "    fill=np.nan)  # Use NaN for no data\n",
    "\n",
    "# Improved BAIS2 classification function with proper NaN handling\n",
    "def classify_bais2(value):\n",
    "    if np.isnan(value):\n",
    "        return 0\n",
    "    elif 0.2 <= value < 0.3:\n",
    "        return 1  # Low\n",
    "    elif 0.3 <= value < 0.4:\n",
    "        return 2  # Moderate\n",
    "    elif 0.4 <= value < 0.45:\n",
    "        return 3  # High\n",
    "    elif value >= 0.45:\n",
    "        return 4  # Extreme\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "bais2_classified = np.vectorize(classify_bais2)(bais2_raster)\n",
    "\n",
    "# Calculate agreement\n",
    "valid_pixels = (qpws_raster > 0) & (bais2_classified > 0)\n",
    "agreement = (qpws_raster[valid_pixels] == bais2_classified[valid_pixels]).sum() / valid_pixels.sum()\n",
    "\n",
    "# Create confusion matrix\n",
    "class_names = ['Low', 'Moderate', 'High', 'Extreme']\n",
    "conf_matrix = confusion_matrix(\n",
    "    qpws_raster[valid_pixels].flatten(),\n",
    "    bais2_classified[valid_pixels].flatten(),\n",
    "    labels=[1, 2, 3, 4]\n",
    ")\n",
    "\n",
    "# Calculate various accuracy metrics\n",
    "def calculate_accuracy_metrics(conf_matrix):\n",
    "    user_acc = np.diag(conf_matrix) / conf_matrix.sum(axis=1)\n",
    "    prod_acc = np.diag(conf_matrix) / conf_matrix.sum(axis=0)\n",
    "    overall_acc = np.trace(conf_matrix) / np.sum(conf_matrix)\n",
    "    \n",
    "    # Calculate confidence intervals (95%)\n",
    "    n = np.sum(conf_matrix)\n",
    "    confidence = 0.95\n",
    "    interval = stats.norm.ppf(1 - (1 - confidence) / 2) * np.sqrt((overall_acc * (1 - overall_acc)) / n)\n",
    "    \n",
    "    # Commission and Omission errors\n",
    "    commission_errors = 1 - user_acc\n",
    "    omission_errors = 1 - prod_acc\n",
    "    \n",
    "    return {\n",
    "        'user_accuracy': user_acc,\n",
    "        'producer_accuracy': prod_acc,\n",
    "        'overall_accuracy': overall_acc,\n",
    "        'confidence_interval': interval,\n",
    "        'commission_errors': commission_errors,\n",
    "        'omission_errors': omission_errors\n",
    "    }\n",
    "\n",
    "metrics = calculate_accuracy_metrics(conf_matrix)\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlOrRd',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.xlabel('BAIS2 Classification')\n",
    "plt.ylabel('QPWS Classification')\n",
    "\n",
    "# 2. Agreement/Disagreement Map\n",
    "plt.subplot(1, 2, 2)\n",
    "agreement_map = np.zeros_like(qpws_raster)\n",
    "agreement_map[valid_pixels] = (qpws_raster[valid_pixels] == bais2_classified[valid_pixels]).astype(int)\n",
    "plt.imshow(agreement_map, cmap='RdYlGn')\n",
    "plt.title('Agreement Map (Green: Agreement, Red: Disagreement)')\n",
    "plt.colorbar(label='Agreement (1) / Disagreement (0)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(f\"\\nOverall Accuracy: {metrics['overall_accuracy']:.2%} (±{metrics['confidence_interval']:.2%})\")\n",
    "print(f\"Kappa Coefficient: {cohen_kappa_score(qpws_raster[valid_pixels].flatten(), bais2_classified[valid_pixels].flatten()):.3f}\")\n",
    "\n",
    "print(\"\\nPer-class Metrics:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"\\n{name} Severity:\")\n",
    "    print(f\"User's Accuracy: {metrics['user_accuracy'][i]:.2%}\")\n",
    "    print(f\"Producer's Accuracy: {metrics['producer_accuracy'][i]:.2%}\")\n",
    "    print(f\"Commission Error: {metrics['commission_errors'][i]:.2%}\")\n",
    "    print(f\"Omission Error: {metrics['omission_errors'][i]:.2%}\")\n",
    "\n",
    "# Perform threshold sensitivity analysis\n",
    "def threshold_sensitivity(bais2_values, qpws_values, threshold_ranges):\n",
    "    results = []\n",
    "    for low_thresh in threshold_ranges['low']:\n",
    "        for mod_thresh in threshold_ranges['moderate']:\n",
    "            for high_thresh in threshold_ranges['high']:\n",
    "                if low_thresh < mod_thresh < high_thresh:\n",
    "                    def temp_classify(value):\n",
    "                        if np.isnan(value):\n",
    "                            return 0\n",
    "                        elif 0.2 <= value < low_thresh:\n",
    "                            return 1\n",
    "                        elif low_thresh <= value < mod_thresh:\n",
    "                            return 2\n",
    "                        elif mod_thresh <= value < high_thresh:\n",
    "                            return 3\n",
    "                        elif value >= high_thresh:\n",
    "                            return 4\n",
    "                        else:\n",
    "                            return 0\n",
    "                    \n",
    "                    classified = np.vectorize(temp_classify)(bais2_values)\n",
    "                    valid = (qpws_values > 0) & (classified > 0)\n",
    "                    accuracy = (qpws_values[valid] == classified[valid]).mean()\n",
    "                    results.append({\n",
    "                        'low_threshold': low_thresh,\n",
    "                        'moderate_threshold': mod_thresh,\n",
    "                        'high_threshold': high_thresh,\n",
    "                        'accuracy': accuracy\n",
    "                    })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define threshold ranges to test\n",
    "threshold_ranges = {\n",
    "    'low': np.arange(0.25, 0.35, 0.05),\n",
    "    'moderate': np.arange(0.35, 0.45, 0.05),\n",
    "    'high': np.arange(0.45, 0.55, 0.05)\n",
    "}\n",
    "\n",
    "sensitivity_results = threshold_sensitivity(bais2_raster, qpws_raster, threshold_ranges)\n",
    "print(\"\\nTop 5 Threshold Combinations:\")\n",
    "print(sensitivity_results.sort_values('accuracy', ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92168bfd-b42f-492a-96a5-f8cd8db3644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio import features\n",
    "import numpy as np\n",
    "from rasterio.transform import from_bounds\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "\n",
    "# First, let's set up our raster parameters\n",
    "resolution = 20\n",
    "\n",
    "# Get the bounds of our study area\n",
    "bounds = qpws_projected.total_bounds\n",
    "width = int((bounds[2] - bounds[0]) / resolution)\n",
    "height = int((bounds[3] - bounds[1]) / resolution)\n",
    "transform = from_bounds(*bounds, width, height)\n",
    "\n",
    "# Create raster for QPWS data\n",
    "qpws_raster = np.zeros((height, width))\n",
    "severity_mapping = {'Low': 1, 'Moderate': 2, 'High': 3, 'Extreme': 4}\n",
    "\n",
    "# Rasterize QPWS data\n",
    "shapes = ((geom, severity_mapping[value]) \n",
    "          for geom, value in zip(qpws_projected.geometry, qpws_projected.Severity))\n",
    "qpws_raster = features.rasterize(shapes=shapes, \n",
    "                                out_shape=(height, width),\n",
    "                                transform=transform,\n",
    "                                fill=0)\n",
    "\n",
    "# Create raster for BAIS2 data\n",
    "bais2_raster = features.rasterize(\n",
    "    [(geom, value) for geom, value in zip(total_burn.geometry, total_burn.attribute)],\n",
    "    out_shape=(height, width),\n",
    "    transform=transform,\n",
    "    fill=np.nan)  # Use NaN for no data\n",
    "\n",
    "# Improved BAIS2 classification function with proper NaN handling\n",
    "def classify_bais2(value):\n",
    "    if np.isnan(value):\n",
    "        return 0\n",
    "    elif 0.2 <= value < 0.3:\n",
    "        return 1  # Low\n",
    "    elif 0.3 <= value < 0.4:\n",
    "        return 2  # Moderate\n",
    "    elif 0.4 <= value < 0.45:\n",
    "        return 3  # High\n",
    "    elif value >= 0.45:\n",
    "        return 4  # Extreme\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "bais2_classified = np.vectorize(classify_bais2)(bais2_raster)\n",
    "\n",
    "# Calculate agreement\n",
    "valid_pixels = (qpws_raster > 0) & (bais2_classified > 0)\n",
    "agreement = (qpws_raster[valid_pixels] == bais2_classified[valid_pixels]).sum() / valid_pixels.sum()\n",
    "\n",
    "# Create confusion matrix\n",
    "class_names = ['Low', 'Moderate', 'High', 'Extreme']\n",
    "conf_matrix = confusion_matrix(\n",
    "    qpws_raster[valid_pixels].flatten(),\n",
    "    bais2_classified[valid_pixels].flatten(),\n",
    "    labels=[1, 2, 3, 4]\n",
    ")\n",
    "\n",
    "# Calculate various accuracy metrics\n",
    "def calculate_accuracy_metrics(conf_matrix):\n",
    "    user_acc = np.diag(conf_matrix) / conf_matrix.sum(axis=1)\n",
    "    prod_acc = np.diag(conf_matrix) / conf_matrix.sum(axis=0)\n",
    "    overall_acc = np.trace(conf_matrix) / np.sum(conf_matrix)\n",
    "    \n",
    "    # Calculate confidence intervals (95%)\n",
    "    n = np.sum(conf_matrix)\n",
    "    confidence = 0.95\n",
    "    interval = stats.norm.ppf(1 - (1 - confidence) / 2) * np.sqrt((overall_acc * (1 - overall_acc)) / n)\n",
    "    \n",
    "    # Commission and Omission errors\n",
    "    commission_errors = 1 - user_acc\n",
    "    omission_errors = 1 - prod_acc\n",
    "    \n",
    "    return {\n",
    "        'user_accuracy': user_acc,\n",
    "        'producer_accuracy': prod_acc,\n",
    "        'overall_accuracy': overall_acc,\n",
    "        'confidence_interval': interval,\n",
    "        'commission_errors': commission_errors,\n",
    "        'omission_errors': omission_errors\n",
    "    }\n",
    "\n",
    "metrics = calculate_accuracy_metrics(conf_matrix)\n",
    "\n",
    "# Create improved visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "fig.suptitle('Comparison of QPWS and BAIS2 Burn Severity Classifications', fontsize=14, y=1.02)\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlOrRd',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax1)\n",
    "ax1.set_title('Confusion Matrix Heatmap')\n",
    "ax1.set_xlabel('BAIS2 Classification')\n",
    "ax1.set_ylabel('QPWS Classification')\n",
    "cbar1 = ax1.collections[0].colorbar\n",
    "cbar1.set_label('Pixel Count', rotation=270, labelpad=15)\n",
    "\n",
    "# 2. Improved Agreement Map with proper masking\n",
    "agreement_map = np.ma.masked_array(np.zeros_like(qpws_raster), mask=True)  # Start with everything masked\n",
    "valid_area = (qpws_raster > 0) | (bais2_classified > 0)  # Areas where either classification exists\n",
    "agreement_map.mask[valid_area] = False  # Unmask only valid areas\n",
    "agreement_map.data[valid_area] = (qpws_raster[valid_area] == bais2_classified[valid_area]).astype(float)\n",
    "\n",
    "# Create custom colormap for agreement map\n",
    "colors = ['#d73027', '#1a9850']  # Red for disagreement, Green for agreement\n",
    "cmap = plt.cm.colors.ListedColormap(colors)\n",
    "\n",
    "# Plot agreement map\n",
    "im = ax2.imshow(agreement_map, cmap=cmap, vmin=0, vmax=1)\n",
    "ax2.set_title('Classification Agreement Map')\n",
    "\n",
    "# Add custom colorbar\n",
    "cbar2 = plt.colorbar(im, ax=ax2)\n",
    "cbar2.set_ticks([0.25, 0.75])\n",
    "cbar2.set_ticklabels(['Disagreement', 'Agreement'])\n",
    "\n",
    "# Add axis labels\n",
    "ax2.set_xlabel('Pixel Column')\n",
    "ax2.set_ylabel('Pixel Row')\n",
    "\n",
    "# Add scale bar\n",
    "scalebar = ScaleBar(resolution, 'm', location='lower left')  # Changed to lower left\n",
    "ax2.add_artist(scalebar)\n",
    "\n",
    "# Add North Arrow\n",
    "ax2.text(0.95, 0.95, '↑N', transform=ax2.transAxes, \n",
    "         fontsize=12, fontweight='bold', \n",
    "         ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the figure\n",
    "# Save the figure\n",
    "plt.savefig('burn_severity_comparison.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32947a60-c0bf-4ac2-8d72-d95bef88f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create figure with subplots\n",
    "fig = plt.figure(figsize=(15, 12))  # Made figure slightly taller\n",
    "plt.suptitle('Burn Severity Classification Accuracy Assessment', fontsize=14, y=0.98)  # Moved title up\n",
    "\n",
    "# Define grid layout with more space between elements\n",
    "gs = fig.add_gridspec(2, 2, width_ratios=[1, 1.2], height_ratios=[0.3, 1], hspace=0.4)  # Increased height_ratios[0] and hspace\n",
    "\n",
    "# Overall metrics in top left\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "overall_data = [['Overall Accuracy', f'{41.57}% (±{0.54}%)'],\n",
    "                ['Kappa Coefficient', f'{0.173:.3f}']]\n",
    "ax1.axis('tight')\n",
    "ax1.axis('off')\n",
    "table1 = ax1.table(cellText=overall_data, \n",
    "                  colWidths=[0.5, 0.5],\n",
    "                  loc='center',\n",
    "                  cellLoc='left')\n",
    "table1.auto_set_font_size(False)\n",
    "table1.set_fontsize(10)\n",
    "table1.scale(1.2, 1.5)\n",
    "\n",
    "# Thresholds table in top right\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "threshold_data = pd.DataFrame({\n",
    "    'Low': [0.25, 0.25, 0.25, 0.25, 0.25],\n",
    "    'Moderate': [0.35, 0.40, 0.35, 0.40, 0.45],\n",
    "    'High': [0.55, 0.55, 0.50, 0.50, 0.55],\n",
    "    'Accuracy': [57.79, 57.30, 56.68, 56.19, 53.58]\n",
    "})\n",
    "ax2.axis('tight')\n",
    "ax2.axis('off')\n",
    "table2 = ax2.table(cellText=threshold_data.values.round(2),\n",
    "                  colLabels=['Low', 'Moderate', 'High', 'Accuracy (%)'],\n",
    "                  loc='center',\n",
    "                  cellLoc='center')\n",
    "table2.auto_set_font_size(False)\n",
    "table2.set_fontsize(10)\n",
    "table2.scale(1.2, 1.5)\n",
    "ax2.set_title('Top 5 Threshold Combinations', pad=20)\n",
    "\n",
    "# Per-class metrics visualization\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "class_metrics = pd.DataFrame({\n",
    "    'Severity Class': ['Low', 'Moderate', 'High', 'Extreme'],\n",
    "    \"User's Accuracy\": [76.99, 40.12, 22.27, 88.26],\n",
    "    \"Producer's Accuracy\": [27.97, 55.78, 66.16, 21.11],\n",
    "    'Commission Error': [23.01, 59.88, 77.73, 11.74],\n",
    "    'Omission Error': [72.03, 44.22, 33.84, 78.89]\n",
    "})\n",
    "\n",
    "# Melt the dataframe for easier plotting\n",
    "melted_metrics = pd.melt(class_metrics, id_vars=['Severity Class'], \n",
    "                        var_name='Metric', value_name='Percentage')\n",
    "\n",
    "# Create grouped bar plot\n",
    "sns.barplot(x='Severity Class', y='Percentage', hue='Metric', \n",
    "            data=melted_metrics, ax=ax3)\n",
    "ax3.set_ylabel('Percentage (%)')\n",
    "ax3.set_title('Per-class Accuracy Metrics')\n",
    "ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('accuracy_assessment.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42449bb0-3435-49b4-858e-91ed47e52f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
